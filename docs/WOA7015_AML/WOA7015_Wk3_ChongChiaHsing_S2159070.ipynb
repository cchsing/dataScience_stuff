{"cells":[{"cell_type":"markdown","metadata":{"id":"OzY5ZsYGoDdu"},"source":["# Welcome to WOA7015 Advance Machine Learning Lab - Week 3\n","This code is generated for the purpose of WOA7015 module.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nVerYxHW-ZrQ"},"source":["# The effect of imbalanced data on AUROC \n","The following code evaluates the effect of imbalanced data on the AUROC of TPR-FPR curve. \n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"c87yzg0goBrP","executionInfo":{"status":"ok","timestamp":1667909196619,"user_tz":-480,"elapsed":2270,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"}}},"outputs":[],"source":["# roc curve and auc on an imbalanced dataset\n","import numpy as np\n","from sklearn.datasets import make_classification\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","import matplotlib.pyplot as plt\n","from imblearn.under_sampling import RandomUnderSampler\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"hcpJntDPEq2J","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1667909229421,"user_tz":-480,"elapsed":311,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"}},"outputId":"7f2b60f1-75a4-48df-9f6f-05b613f0788b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.32584935  0.21897754  0.62061895 ...  2.84071377 -0.02582733\n","  -0.40885762]\n"," [-1.12624124 -0.86026727 -0.89264356 ... -0.92962064  0.59483549\n","   1.24052468]\n"," [-0.48993428 -0.7453348  -1.43801838 ... -1.67525801 -0.09994425\n","  -0.46569289]\n"," ...\n"," [ 0.47406074 -1.9209351   0.41681779 ...  1.04574815  1.092832\n","  -0.01541749]\n"," [-0.62731673 -0.94336697 -1.50694171 ... -0.85092941  0.99046917\n","   2.19583454]\n"," [ 0.88990126  0.81857103 -2.12551556 ...  1.00271323 -0.88101446\n","  -0.81149645]]\n","-------------------------------------------------------------------------------------------\n","[1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0\n"," 0 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 1\n"," 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1\n"," 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0\n"," 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1\n"," 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0\n"," 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 1 0\n"," 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0\n"," 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0\n"," 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0\n"," 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0\n"," 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0\n"," 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1\n"," 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0\n"," 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0\n"," 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0\n"," 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1\n"," 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0\n"," 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1\n"," 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1\n"," 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0\n"," 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1\n"," 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0\n"," 1 0 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0\n"," 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0\n"," 0]\n"]}],"source":["# generate 2 class dataset \n","X, y = make_classification(n_samples=1000, n_classes=2, random_state=1000)\n","\n","print(X)\n","print('-------------------------------------------------------------------------------------------')\n","print(y)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":289,"status":"ok","timestamp":1667909352482,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"},"user_tz":-480},"id":"wLo12OKVE__J","outputId":"6e396a0b-b04f-484d-ecef-d17e6cc452ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["trainy - class0:  253\n","trainy - class1:  247\n","---------------------------------------------------------------------\n","testy - class0:  249\n","testy - class1:  251\n","=====================================================================\n","Balanced Testing date\n","testy - class0:  249\n","testy - class1:  249\n"]}],"source":["# split into train/testhu sets\n","trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=1000)\n","\n","print('trainy - class0: ', len(trainy)-trainy.sum())\n","print('trainy - class1: ', trainy.sum())\n","print('---------------------------------------------------------------------')\n","print('testy - class0: ', len(testy)-testy.sum())\n","print('testy - class1: ', testy.sum())\n","print('=====================================================================')\n","\n","# make testing dataset balance\n","undersample = RandomUnderSampler(sampling_strategy='majority')\n","testX, testy = undersample.fit_resample(testX, testy)\n","\n","print('Balanced Testing date')\n","print('testy - class0: ', len(testy)-testy.sum())\n","print('testy - class1: ', testy.sum())"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Y3QmySaLE7Nm","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1667909360786,"user_tz":-480,"elapsed":306,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"}},"outputId":"31f103dd-4653-46a6-ce4e-c460fa952cf1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression()"]},"metadata":{},"execution_count":6}],"source":["# fit a model with training data\n","model = LogisticRegression(solver='lbfgs')\n","model.fit(trainX, trainy)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"nOu_783uGpZd","colab":{"base_uri":"https://localhost:8080/","height":660},"executionInfo":{"status":"ok","timestamp":1667915465152,"user_tz":-480,"elapsed":975,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"}},"outputId":"96ddaeab-59d5-4930-df5b-21718c1cc08b"},"outputs":[{"output_type":"stream","name":"stdout","text":["nth 0:positive: 249 negative: 249\n","---------------------------------------------\n","nth 1:positive: 225 negative: 249\n","---------------------------------------------\n","nth 2:positive: 200 negative: 249\n","---------------------------------------------\n","nth 3:positive: 175 negative: 249\n","---------------------------------------------\n","nth 4:positive: 150 negative: 249\n","---------------------------------------------\n","nth 5:positive: 125 negative: 249\n","---------------------------------------------\n","nth 6:positive: 100 negative: 249\n","---------------------------------------------\n","nth 7:positive: 75 negative: 249\n","---------------------------------------------\n","nth 8:positive: 50 negative: 249\n","---------------------------------------------\n","nth 9:positive: 25 negative: 249\n","---------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'decreasing positive sample')"]},"metadata":{},"execution_count":7},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# repeat with different skewness \n","roc_list = []\n","lr_acc = []\n","k=1\n","for i in range(0, 10):\n","  pos_ind = np.where(testy==1)[0]\n","  n = int(i/10 * len(pos_ind))\n","  tmp_testX, tmp_testy = np.copy(testX), np.copy(testy)\n","  tmp_testX = np.delete(tmp_testX, pos_ind[:n], axis=0)\n","  tmp_testy = np.delete(tmp_testy, pos_ind[:n], axis=0)\n","  print('nth %d:positive: %d negative: %d' \n","        % (i, tmp_testy.sum(), tmp_testy.shape[0] - tmp_testy.sum()))\n","  print('---------------------------------------------')\n","  \n","  # predict probabilities\n","  lr_pred = model.predict(tmp_testX)\n","  lr_probs = model.predict_proba(tmp_testX)\n","  # keep probabilities for the positive outcome only\n","  lr_probs = lr_probs[:, 1]\n","  # calculate scores\n","  lr_auc = roc_auc_score(tmp_testy, lr_probs)\n","\n","  # summarize scores\n","  # print('iteration %d: Logistic: ROC AUC=%.3f' % (k, lr_auc))\n","  k += 1\n","  # calculate roc curves\n","  lr_fpr, lr_tpr, _ = roc_curve(tmp_testy, lr_probs)\n","  roc_list.append(lr_auc)\n","\n","plt.plot(np.arange(0, len(roc_list)), roc_list)\n","plt.xlabel('skewness ratio')\n","plt.ylabel('AUROC')\n","plt.title('decreasing positive sample')\n"]},{"cell_type":"markdown","metadata":{"id":"njFP2GHpC1VE"},"source":["# Exercise 1:\n","\n","Does the AUROC (TPR vs FPR) affected by imbalanced class?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"boJtP8JnwMeB"},"source":["\n","---\n","From the graph generated, the AUROC varies between 0.949 to 0.954 with a oscillating characteristics as the skewness ratio increases. It could be said that the AUROC is not affected by the imbalanced class. <br>\n","This conclusion can be supported by an article, [Unbalanced Data? Stop Using ROC-AUC and Use AUPRC Instead](https://towardsdatascience.com/imbalanced-data-stop-using-roc-auc-and-use-auprc-instead-46af4910a494), by Daniel Rosenberg where it is shown that the PRC can measure the performance of a model much better than ROC when the data set is imbalanced. <br>\n","Within the article, it was further explained that the insensitivity of the ROC is attributed to the FPR where the change is not noticeable, \"The change in FPR is slow compared to the change in recall, given unbalanced data. This factor drives all the difference.\""]},{"cell_type":"markdown","metadata":{"id":"KK4Cxp0q75PM"},"source":["# The effect of imbalanced data on AUROC of PR curve and F1 score\n","\n","> Блок с отступами\n","\n","\n","The following code evaluates the effect of imbalanced data on the AUROC of Precision-Recall and F1 value. \n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"oYxjJuD_8ewJ","executionInfo":{"status":"ok","timestamp":1667921055359,"user_tz":-480,"elapsed":317,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"}}},"outputs":[],"source":["# roc curve and auc on an imbalanced dataset\n","import numpy as np\n","from sklearn.datasets import make_classification\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import auc, f1_score\n","from sklearn.metrics import precision_recall_curve\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"nr_uY_mPHLTF","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1667921065648,"user_tz":-480,"elapsed":516,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"}},"outputId":"2449ab89-17c4-4da2-f4f0-2d524de6a2fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.32584935  0.21897754  0.62061895 ...  2.84071377 -0.02582733\n","  -0.40885762]\n"," [-1.12624124 -0.86026727 -0.89264356 ... -0.92962064  0.59483549\n","   1.24052468]\n"," [-0.48993428 -0.7453348  -1.43801838 ... -1.67525801 -0.09994425\n","  -0.46569289]\n"," ...\n"," [ 0.47406074 -1.9209351   0.41681779 ...  1.04574815  1.092832\n","  -0.01541749]\n"," [-0.62731673 -0.94336697 -1.50694171 ... -0.85092941  0.99046917\n","   2.19583454]\n"," [ 0.88990126  0.81857103 -2.12551556 ...  1.00271323 -0.88101446\n","  -0.81149645]]\n","-----------\n","[1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 0\n"," 0 0 0 1 1 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 0\n"," 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 0 1\n"," 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 1\n"," 0 0 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0\n"," 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 0\n"," 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1\n"," 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0\n"," 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 1 1 0\n"," 1 1 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 0\n"," 1 1 0 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0\n"," 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0\n"," 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0\n"," 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 0 0\n"," 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1\n"," 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0\n"," 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0\n"," 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0\n"," 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1\n"," 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0\n"," 0 0 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1\n"," 1 1 0 0 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1\n"," 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0\n"," 0 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1\n"," 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0\n"," 1 0 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0\n"," 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 1 1 1 0\n"," 0]\n"]}],"source":["# generate 2 class dataset \n","X, y = make_classification(n_samples=1000, n_classes=2, random_state=1000)\n","\n","print(X)\n","print('-----------')\n","print(y)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":319,"status":"ok","timestamp":1667921211317,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"},"user_tz":-480},"id":"bbmv6pFhHWyQ","outputId":"79fe26c3-b104-494c-a47e-e5bf019a942d"},"outputs":[{"output_type":"stream","name":"stdout","text":["trainy - class0:  253\n","trainy - class1:  247\n","----------------------\n","testy - class0:  249\n","testy - class1:  251\n","============================\n","Balanced Testing date\n","testy - class0:  249\n","testy - class1:  249\n"]}],"source":["# split into train/test sets\n","trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=1000)\n","\n","print('trainy - class0: ', len(trainy)-trainy.sum())\n","print('trainy - class1: ', trainy.sum())\n","print('----------------------')\n","print('testy - class0: ', len(testy)-testy.sum())\n","print('testy - class1: ', testy.sum())\n","print('============================')\n","\n","# make testing dataset balance\n","undersample = RandomUnderSampler(sampling_strategy='majority')\n","testX, testy = undersample.fit_resample(testX, testy)\n","\n","print('Balanced Testing date')\n","print('testy - class0: ', len(testy)-testy.sum())\n","print('testy - class1: ', testy.sum())\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":316,"status":"ok","timestamp":1667921219728,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"},"user_tz":-480},"id":"i1ST-YEcHh5I","outputId":"935e4c84-6af6-4d5d-ea3e-0e7934ba2077"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression()"]},"metadata":{},"execution_count":11}],"source":["# fit a model\n","model = LogisticRegression(solver='lbfgs')\n","model.fit(trainX, trainy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"scPocodDHRp-","colab":{"base_uri":"https://localhost:8080/","height":947},"executionInfo":{"status":"ok","timestamp":1667795484865,"user_tz":-480,"elapsed":976,"user":{"displayName":"ZI HUI","userId":"14151178838169878984"}},"outputId":"1aa9180e-a1c0-4a03-988b-e54848129de0"},"outputs":[{"output_type":"stream","name":"stdout","text":["nth 0:positive: 249 negative: 249\n","---------------------------------------------\n","nth 1:positive: 225 negative: 249\n","---------------------------------------------\n","nth 2:positive: 200 negative: 249\n","---------------------------------------------\n","nth 3:positive: 175 negative: 249\n","---------------------------------------------\n","nth 4:positive: 150 negative: 249\n","---------------------------------------------\n","nth 5:positive: 125 negative: 249\n","---------------------------------------------\n","nth 6:positive: 100 negative: 249\n","---------------------------------------------\n","nth 7:positive: 75 negative: 249\n","---------------------------------------------\n","nth 8:positive: 50 negative: 249\n","---------------------------------------------\n","nth 9:positive: 25 negative: 249\n","---------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'decreasing positive sample')"]},"metadata":{},"execution_count":13},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# repeat with different skewness \n","roc_list = []\n","f1_list = []\n","\n","k=1\n","for i in range(0, 10):\n","  pos_ind = np.where(testy==1)[0]\n","  n = int(i/10 * len(pos_ind))\n","  tmp_testX, tmp_testy = np.copy(testX), np.copy(testy)\n","  tmp_testX = np.delete(tmp_testX, pos_ind[:n], axis=0)\n","  tmp_testy = np.delete(tmp_testy, pos_ind[:n], axis=0)\n","  print('nth %d:positive: %d negative: %d' \n","        % (i, tmp_testy.sum(), tmp_testy.shape[0] - tmp_testy.sum()))\n","  print('---------------------------------------------')\n","  \n","\n","  # predict probabilities\n","  lr_probs = model.predict_proba(tmp_testX)\n","  # keep probabilities for the positive outcome only\n","  lr_probs = lr_probs[:, 1]\n","  # predict class values\n","  yhat = model.predict(tmp_testX)\n","  # calculate precision and recall for each threshold\n","  lr_precision, lr_recall, _ = precision_recall_curve(tmp_testy, lr_probs)\n","  # calculate scores \n","  lr_f1, lr_auc = f1_score(tmp_testy, yhat), auc(lr_recall, lr_precision)\n","  # summarize scores\n","  # print('iteration%d Logistic: f1=%.3f auc=%.3f' % (k, lr_f1, lr_auc))\n","  k += 1\n","  roc_list.append(lr_auc)\n","  f1_list.append(lr_f1)\n","\n","plt.plot(np.arange(0, len(roc_list)), roc_list)\n","plt.xlabel('skewness ratio')\n","plt.ylabel('AUC of PR curve')\n","plt.title('decreasing positive sample')\n","\n","plt.figure()\n","plt.plot(np.arange(0, len(roc_list)), f1_list)\n","plt.xlabel('skewness ratio')\n","plt.ylabel('F1')\n","plt.title('decreasing positive sample')\n"]},{"cell_type":"markdown","metadata":{"id":"2DP3LOZY7v6M"},"source":["# Exercise 2 (4%):\n","Does the AUROC (Precision vs Recall), F1 score affected by imbalanced class?"]},{"cell_type":"markdown","metadata":{"id":"fixmonZOA7fJ"},"source":["---\n","From the graph of AUPRC, it can be seen that the value of the AUPRC decrease from around 0.9 to around 0.5 as the skewness ratio increase from 0 to 8. It is obvious that the AUPRC is affected by the imbalanced of data set. <br>\n","Similarly, it is seen that the F1 score also decreases from 0.9 to 0.65 as the skewness ratio increases from 0 to 8. <br>\n","Thus, both the AUPRC and F1 score are sensitive to measuring the performance of model when the data set is imbalance. This might attribute to the measurement of FN and FP.<br><br>\n","$Recall = {TP \\over TP + FN}$ <br>\n","$Precision = {TP \\over TP + FP}$ <br>\n","$F1 = {TP \\over TP + {1 \\over 2} (FP + FN)}$"]},{"cell_type":"markdown","metadata":{"id":"_h1qUoy4DSc2"},"source":["# ***Let's go back to power point - slide 15***"]},{"cell_type":"markdown","metadata":{"id":"SK3GJxwgzEjP"},"source":["# Convex function\n","\n","This is the code to generate the graph in slide 38"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3v0il6m3zOQb"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import imageio\n","\n","x = np.arange(-2, 2, 0.01)\n","\n","# choose one function to try\n","# f = lambda x: 0.5 * x ** 2 # Convex\n","#f = lambda x: np.cos(np.pi * x)  # Nonconvex\n","f = lambda x: -0.5 * x ** 4  # Nonconvex\n","\n","filenames=[]\n","for lamda in np.arange(0, 1, 0.02):\n","  # LHS\n","  tmp_x = lamda*x[0] + (1-lamda)*x[-1]\n","\n","  # RHS\n","  x_line, y_line = np.array([x[0], x[-1]]), np.array([lamda*f(x[0]), (1-lamda)*f(x[-1])])\n","\n","  # compute LHS and RHS\n","  LHS = f(tmp_x)\n","  RHS = lamda*f(x[0]) + (1-lamda)*f(x[-1])\n","  if LHS > RHS:\n","    print('At lamda %0.3f, it is concave' % lamda)\n","    print('lhs %.5f rhs %.5f' % (LHS, RHS))\n","\n","  plt.figure()\n","  # original graph\n","  plt.plot(x, f(x), label='f(x)')\n","  # plot RHS\n","  plt.plot(x_line, y_line, label='%0.3f' % lamda)\n","  # plot LHS\n","  plt.scatter(tmp_x, f(tmp_x))\n","  #title, legennd\n","  plt.title('lhs %.3f rhs %.3f' % (LHS, RHS))\n","  plt.legend()\n","  plt.savefig('lamda %0.3f.png' % lamda)\n","  # plt.close()\n","  filenames.append('lamda %0.3f.png' % lamda)\n","\n","# Build GIF\n","with imageio.get_writer('mygif.gif', mode='I') as writer:\n","    for filename in filenames:\n","        image = imageio.imread(filename)\n","        writer.append_data(image)"]},{"cell_type":"markdown","metadata":{"id":"l9Wuu9o0pGmg"},"source":["# Understand how learning rate affects your SGD optimization\n","\n","We will train a neural network for a pretty simple task, i.e. calculating the exclusive-or (XOR) of two input. \n","\n","<br> \n","<img src=\"https://raw.githubusercontent.com/shiernee/Advanced_ML/main/Week3/XOR.jpg\" width=\"512\"/>\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Qr2IABWKoGHk","executionInfo":{"status":"ok","timestamp":1667926021730,"user_tz":-480,"elapsed":300,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"}}},"outputs":[],"source":["import random\n","import numpy as np"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":288,"status":"ok","timestamp":1667926058779,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"},"user_tz":-480},"id":"3PpJZLhyndlE","outputId":"93957bb6-4f44-43e9-cf42-d9b7cb2a6987"},"outputs":[{"output_type":"stream","name":"stdout","text":["x1: 1\n","x2: 1\n","yy: 0\n"]}],"source":["# generate a function for XOR\n","x1 = random.randint(0, 1)\n","x2 = random.randint(0, 1)\n","yy = 0 if (x1 == x2) else 1\n","\n","print('x1:', x1)\n","print('x2:',x2)\n","print('yy:',yy)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1667926099786,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"},"user_tz":-480},"id":"sucvJe_fntPU","outputId":"0150ca6a-0218-41e4-e2a4-c4693f161dc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["x1: 1\n","x2: 1\n","yy: 0\n","after centered\n","x1: 1.0\n","x2: 1.0\n","yy: -1.0\n"]}],"source":["x1 = random.randint(0, 1)\n","x2 = random.randint(0, 1)\n","yy = 0 if (x1 == x2) else 1\n","\n","print('x1:', x1)\n","print('x2:',x2)\n","print('yy:',yy)\n","\n","# centered at zero\n","x1 = 2. * (x1 - 0.5)\n","x2 = 2. * (x2 - 0.5)\n","yy = 2. * (yy - 0.5)\n","\n","print('after centered')\n","print('x1:', x1)\n","print('x2:',x2)\n","print('yy:',yy)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":296,"status":"ok","timestamp":1667926122732,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"},"user_tz":-480},"id":"aEbi2kK4n9Ci","outputId":"16b7a2d7-a002-46f8-f234-46d31cb74c67"},"outputs":[{"output_type":"stream","name":"stdout","text":["x1: 1.0917544011760465\n","x2: 1.0767232845136954\n","yy: -0.9013353551788232\n"]}],"source":["x1 = random.randint(0, 1)\n","x2 = random.randint(0, 1)\n","yy = 0 if (x1 == x2) else 1\n","\n","# centered at zero\n","x1 = 2. * (x1 - 0.5)\n","x2 = 2. * (x2 - 0.5)\n","yy = 2. * (yy - 0.5)\n","\n","# add noise\n","x1 += 0.1 * random.random()\n","x2 += 0.1 * random.random()\n","yy += 0.1 * random.random()\n","\n","print('x1:', x1)\n","print('x2:',x2)\n","print('yy:',yy)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"6RiENlDbpGS3","executionInfo":{"status":"ok","timestamp":1667926140410,"user_tz":-480,"elapsed":671,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"}}},"outputs":[],"source":["# make it into function \n","def make_data():\n","    x1 = random.randint(0, 1)\n","    x2 = random.randint(0, 1)\n","    yy = 0 if (x1 == x2) else 1\n"," \n","    # centered at zero\n","    x1 = 2. * (x1 - 0.5)\n","    x2 = 2. * (x2 - 0.5)\n","    yy = 2. * (yy - 0.5)\n"," \n","    # add noise\n","    x1 += 0.1 * random.random()\n","    x2 += 0.1 * random.random()\n","    yy += 0.1 * random.random()\n"," \n","    return [x1, x2, ], yy\n"," "]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":364,"status":"ok","timestamp":1667926173362,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"},"user_tz":-480},"id":"IRS81-dioL4n","outputId":"49722192-bc8b-44fb-d38a-c76f6726413e"},"outputs":[{"output_type":"stream","name":"stdout","text":["(array([[-0.9044449 , -0.9459794 ],\n","       [-0.9042933 , -0.9261682 ],\n","       [-0.9249838 , -0.9761731 ],\n","       [-0.9460286 , -0.9093589 ],\n","       [ 1.0085515 ,  1.0911613 ],\n","       [-0.99794984,  1.0014544 ],\n","       [-0.918716  , -0.9379624 ],\n","       [ 1.0058728 , -0.99944526],\n","       [ 1.0941318 , -0.9932331 ],\n","       [-0.9272939 ,  1.0603557 ]], dtype=float32), array([-0.97914565, -0.9668333 , -0.9084225 , -0.9061493 , -0.9348135 ,\n","        1.0026252 , -0.9488156 ,  1.0135399 ,  1.0740955 ,  1.0869725 ],\n","      dtype=float32))\n"]}],"source":["# create batch samples\n","batch_size = 10\n","def make_batch():\n","    data = [make_data() for ii in range(batch_size)]\n","    labels = [label for xx, label in data]\n","    data = [xx for xx, label in data]\n","    return np.array(data, dtype='float32'), np.array(labels, dtype='float32')\n"," \n","print(make_batch())\n"]},{"cell_type":"code","execution_count":79,"metadata":{"id":"XB7kASWVoZJi","executionInfo":{"status":"ok","timestamp":1667927588828,"user_tz":-480,"elapsed":353,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"}}},"outputs":[],"source":["# generate  500 train and 50 test data \n","train_data = [make_batch() for ii in range(500)]\n","test_data = [make_batch() for ii in range(50)]\n"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"huxK2x7WpUGw","executionInfo":{"status":"ok","timestamp":1667927590553,"user_tz":-480,"elapsed":300,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"}}},"outputs":[],"source":["# import torch libraries\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n"," "]},{"cell_type":"code","execution_count":81,"metadata":{"id":"BMj_0PO0ojry","executionInfo":{"status":"ok","timestamp":1667927592028,"user_tz":-480,"elapsed":2,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"}}},"outputs":[],"source":["## Define our neural network class\n","torch.manual_seed(42)\n"," \n","class NN(nn.Module):\n","    def __init__(self):\n","        super(NN, self).__init__()\n"," \n","        self.dense1 = nn.Linear(2, 2)\n","        self.dense2 = nn.Linear(2, 1)\n"," \n","    def forward(self, x):\n","        x = F.tanh(self.dense1(x))\n","        x = self.dense2(x)\n","        return torch.squeeze(x)\n"," \n"]},{"cell_type":"code","execution_count":107,"metadata":{"id":"ZqLyWCLOtrJS","executionInfo":{"status":"ok","timestamp":1667927989810,"user_tz":-480,"elapsed":284,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"}}},"outputs":[],"source":["# initialize our network\n","model = NN()\n","\n","lr = 0.1\n","## optimizer = stochastic gradient descent\n","optimizer = optim.SGD(model.parameters(), lr)"]},{"cell_type":"code","execution_count":108,"metadata":{"id":"Furjt7ZppdxA","executionInfo":{"status":"ok","timestamp":1667927991205,"user_tz":-480,"elapsed":1,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"}}},"outputs":[],"source":["## train and test functions\n"," \n","def train(epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_data):\n","        data, target = Variable(torch.from_numpy(data)), Variable(torch.from_numpy(target))\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.mse_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 100 == 0:\n","            print('Train Epoch: {} {}\\tLoss: {:.4f}'.format(epoch, batch_idx * len(data), loss.item()))\n"," \n","def test():\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    for data, target in test_data:\n","        data, target = Variable(torch.from_numpy(data), volatile=True), Variable(torch.from_numpy(target))\n","        output = model(data)\n","        test_loss += F.mse_loss(output, target)\n","        correct += (np.around(output.data.numpy()) == np.around(target.data.numpy())).sum()\n"," \n","    test_loss /= len(test_data)\n","    test_loss = test_loss.item()\n"," \n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        test_loss, correct, batch_size * len(test_data), 100. * correct / (batch_size * len(test_data))) )\n"," "]},{"cell_type":"code","execution_count":109,"metadata":{"id":"wZ4c_vUzphKy","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1667928025634,"user_tz":-480,"elapsed":32374,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"}},"outputId":"1622a24f-eb6c-4866-9045-3f4933e8504d"},"outputs":[{"output_type":"stream","name":"stdout","text":["lr= 0.1\n","Train Epoch: 1 0\tLoss: 1.3039\n","Train Epoch: 1 1000\tLoss: 0.0565\n","Train Epoch: 1 2000\tLoss: 0.0007\n","Train Epoch: 1 3000\tLoss: 0.0010\n","Train Epoch: 1 4000\tLoss: 0.0011\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0010, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 2 0\tLoss: 0.0013\n","Train Epoch: 2 1000\tLoss: 0.0009\n","Train Epoch: 2 2000\tLoss: 0.0007\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"]},{"output_type":"stream","name":"stdout","text":["Train Epoch: 2 3000\tLoss: 0.0010\n","Train Epoch: 2 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0010, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 3 0\tLoss: 0.0013\n","Train Epoch: 3 1000\tLoss: 0.0009\n","Train Epoch: 3 2000\tLoss: 0.0007\n","Train Epoch: 3 3000\tLoss: 0.0010\n","Train Epoch: 3 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0010, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 4 0\tLoss: 0.0012\n","Train Epoch: 4 1000\tLoss: 0.0009\n","Train Epoch: 4 2000\tLoss: 0.0007\n","Train Epoch: 4 3000\tLoss: 0.0010\n","Train Epoch: 4 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0010, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 5 0\tLoss: 0.0012\n","Train Epoch: 5 1000\tLoss: 0.0009\n","Train Epoch: 5 2000\tLoss: 0.0008\n","Train Epoch: 5 3000\tLoss: 0.0010\n","Train Epoch: 5 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0010, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 6 0\tLoss: 0.0011\n","Train Epoch: 6 1000\tLoss: 0.0009\n","Train Epoch: 6 2000\tLoss: 0.0008\n","Train Epoch: 6 3000\tLoss: 0.0010\n","Train Epoch: 6 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0010, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 7 0\tLoss: 0.0011\n","Train Epoch: 7 1000\tLoss: 0.0009\n","Train Epoch: 7 2000\tLoss: 0.0008\n","Train Epoch: 7 3000\tLoss: 0.0009\n","Train Epoch: 7 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0010, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 8 0\tLoss: 0.0011\n","Train Epoch: 8 1000\tLoss: 0.0009\n","Train Epoch: 8 2000\tLoss: 0.0008\n","Train Epoch: 8 3000\tLoss: 0.0009\n","Train Epoch: 8 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0010, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 9 0\tLoss: 0.0011\n","Train Epoch: 9 1000\tLoss: 0.0009\n","Train Epoch: 9 2000\tLoss: 0.0008\n","Train Epoch: 9 3000\tLoss: 0.0009\n","Train Epoch: 9 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0010, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 10 0\tLoss: 0.0011\n","Train Epoch: 10 1000\tLoss: 0.0009\n","Train Epoch: 10 2000\tLoss: 0.0008\n","Train Epoch: 10 3000\tLoss: 0.0009\n","Train Epoch: 10 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0010, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 11 0\tLoss: 0.0011\n","Train Epoch: 11 1000\tLoss: 0.0009\n","Train Epoch: 11 2000\tLoss: 0.0008\n","Train Epoch: 11 3000\tLoss: 0.0009\n","Train Epoch: 11 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0010, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 12 0\tLoss: 0.0010\n","Train Epoch: 12 1000\tLoss: 0.0009\n","Train Epoch: 12 2000\tLoss: 0.0008\n","Train Epoch: 12 3000\tLoss: 0.0009\n","Train Epoch: 12 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 13 0\tLoss: 0.0010\n","Train Epoch: 13 1000\tLoss: 0.0009\n","Train Epoch: 13 2000\tLoss: 0.0008\n","Train Epoch: 13 3000\tLoss: 0.0009\n","Train Epoch: 13 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 14 0\tLoss: 0.0010\n","Train Epoch: 14 1000\tLoss: 0.0009\n","Train Epoch: 14 2000\tLoss: 0.0008\n","Train Epoch: 14 3000\tLoss: 0.0009\n","Train Epoch: 14 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 15 0\tLoss: 0.0010\n","Train Epoch: 15 1000\tLoss: 0.0009\n","Train Epoch: 15 2000\tLoss: 0.0008\n","Train Epoch: 15 3000\tLoss: 0.0009\n","Train Epoch: 15 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 16 0\tLoss: 0.0010\n","Train Epoch: 16 1000\tLoss: 0.0009\n","Train Epoch: 16 2000\tLoss: 0.0008\n","Train Epoch: 16 3000\tLoss: 0.0009\n","Train Epoch: 16 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 17 0\tLoss: 0.0010\n","Train Epoch: 17 1000\tLoss: 0.0009\n","Train Epoch: 17 2000\tLoss: 0.0008\n","Train Epoch: 17 3000\tLoss: 0.0009\n","Train Epoch: 17 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 18 0\tLoss: 0.0010\n","Train Epoch: 18 1000\tLoss: 0.0009\n","Train Epoch: 18 2000\tLoss: 0.0008\n","Train Epoch: 18 3000\tLoss: 0.0009\n","Train Epoch: 18 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 19 0\tLoss: 0.0010\n","Train Epoch: 19 1000\tLoss: 0.0009\n","Train Epoch: 19 2000\tLoss: 0.0009\n","Train Epoch: 19 3000\tLoss: 0.0009\n","Train Epoch: 19 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 20 0\tLoss: 0.0010\n","Train Epoch: 20 1000\tLoss: 0.0009\n","Train Epoch: 20 2000\tLoss: 0.0009\n","Train Epoch: 20 3000\tLoss: 0.0009\n","Train Epoch: 20 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 21 0\tLoss: 0.0010\n","Train Epoch: 21 1000\tLoss: 0.0009\n","Train Epoch: 21 2000\tLoss: 0.0009\n","Train Epoch: 21 3000\tLoss: 0.0009\n","Train Epoch: 21 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 22 0\tLoss: 0.0010\n","Train Epoch: 22 1000\tLoss: 0.0009\n","Train Epoch: 22 2000\tLoss: 0.0009\n","Train Epoch: 22 3000\tLoss: 0.0009\n","Train Epoch: 22 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 23 0\tLoss: 0.0010\n","Train Epoch: 23 1000\tLoss: 0.0009\n","Train Epoch: 23 2000\tLoss: 0.0009\n","Train Epoch: 23 3000\tLoss: 0.0009\n","Train Epoch: 23 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 24 0\tLoss: 0.0010\n","Train Epoch: 24 1000\tLoss: 0.0009\n","Train Epoch: 24 2000\tLoss: 0.0009\n","Train Epoch: 24 3000\tLoss: 0.0009\n","Train Epoch: 24 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 25 0\tLoss: 0.0010\n","Train Epoch: 25 1000\tLoss: 0.0009\n","Train Epoch: 25 2000\tLoss: 0.0009\n","Train Epoch: 25 3000\tLoss: 0.0009\n","Train Epoch: 25 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 26 0\tLoss: 0.0010\n","Train Epoch: 26 1000\tLoss: 0.0009\n","Train Epoch: 26 2000\tLoss: 0.0009\n","Train Epoch: 26 3000\tLoss: 0.0009\n","Train Epoch: 26 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 27 0\tLoss: 0.0010\n","Train Epoch: 27 1000\tLoss: 0.0009\n","Train Epoch: 27 2000\tLoss: 0.0009\n","Train Epoch: 27 3000\tLoss: 0.0009\n","Train Epoch: 27 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 28 0\tLoss: 0.0010\n","Train Epoch: 28 1000\tLoss: 0.0009\n","Train Epoch: 28 2000\tLoss: 0.0009\n","Train Epoch: 28 3000\tLoss: 0.0009\n","Train Epoch: 28 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 29 0\tLoss: 0.0010\n","Train Epoch: 29 1000\tLoss: 0.0009\n","Train Epoch: 29 2000\tLoss: 0.0009\n","Train Epoch: 29 3000\tLoss: 0.0009\n","Train Epoch: 29 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 30 0\tLoss: 0.0010\n","Train Epoch: 30 1000\tLoss: 0.0009\n","Train Epoch: 30 2000\tLoss: 0.0009\n","Train Epoch: 30 3000\tLoss: 0.0009\n","Train Epoch: 30 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 31 0\tLoss: 0.0010\n","Train Epoch: 31 1000\tLoss: 0.0009\n","Train Epoch: 31 2000\tLoss: 0.0009\n","Train Epoch: 31 3000\tLoss: 0.0009\n","Train Epoch: 31 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 32 0\tLoss: 0.0010\n","Train Epoch: 32 1000\tLoss: 0.0009\n","Train Epoch: 32 2000\tLoss: 0.0009\n","Train Epoch: 32 3000\tLoss: 0.0009\n","Train Epoch: 32 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 33 0\tLoss: 0.0010\n","Train Epoch: 33 1000\tLoss: 0.0009\n","Train Epoch: 33 2000\tLoss: 0.0009\n","Train Epoch: 33 3000\tLoss: 0.0009\n","Train Epoch: 33 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 34 0\tLoss: 0.0010\n","Train Epoch: 34 1000\tLoss: 0.0009\n","Train Epoch: 34 2000\tLoss: 0.0009\n","Train Epoch: 34 3000\tLoss: 0.0009\n","Train Epoch: 34 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 35 0\tLoss: 0.0010\n","Train Epoch: 35 1000\tLoss: 0.0009\n","Train Epoch: 35 2000\tLoss: 0.0009\n","Train Epoch: 35 3000\tLoss: 0.0009\n","Train Epoch: 35 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 36 0\tLoss: 0.0010\n","Train Epoch: 36 1000\tLoss: 0.0009\n","Train Epoch: 36 2000\tLoss: 0.0009\n","Train Epoch: 36 3000\tLoss: 0.0009\n","Train Epoch: 36 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 37 0\tLoss: 0.0010\n","Train Epoch: 37 1000\tLoss: 0.0009\n","Train Epoch: 37 2000\tLoss: 0.0009\n","Train Epoch: 37 3000\tLoss: 0.0009\n","Train Epoch: 37 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 38 0\tLoss: 0.0010\n","Train Epoch: 38 1000\tLoss: 0.0009\n","Train Epoch: 38 2000\tLoss: 0.0009\n","Train Epoch: 38 3000\tLoss: 0.0009\n","Train Epoch: 38 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 39 0\tLoss: 0.0010\n","Train Epoch: 39 1000\tLoss: 0.0009\n","Train Epoch: 39 2000\tLoss: 0.0009\n","Train Epoch: 39 3000\tLoss: 0.0009\n","Train Epoch: 39 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 40 0\tLoss: 0.0010\n","Train Epoch: 40 1000\tLoss: 0.0009\n","Train Epoch: 40 2000\tLoss: 0.0009\n","Train Epoch: 40 3000\tLoss: 0.0009\n","Train Epoch: 40 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 41 0\tLoss: 0.0010\n","Train Epoch: 41 1000\tLoss: 0.0009\n","Train Epoch: 41 2000\tLoss: 0.0009\n","Train Epoch: 41 3000\tLoss: 0.0009\n","Train Epoch: 41 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 42 0\tLoss: 0.0010\n","Train Epoch: 42 1000\tLoss: 0.0009\n","Train Epoch: 42 2000\tLoss: 0.0009\n","Train Epoch: 42 3000\tLoss: 0.0009\n","Train Epoch: 42 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 43 0\tLoss: 0.0010\n","Train Epoch: 43 1000\tLoss: 0.0009\n","Train Epoch: 43 2000\tLoss: 0.0009\n","Train Epoch: 43 3000\tLoss: 0.0009\n","Train Epoch: 43 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 44 0\tLoss: 0.0010\n","Train Epoch: 44 1000\tLoss: 0.0009\n","Train Epoch: 44 2000\tLoss: 0.0009\n","Train Epoch: 44 3000\tLoss: 0.0009\n","Train Epoch: 44 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 45 0\tLoss: 0.0010\n","Train Epoch: 45 1000\tLoss: 0.0009\n","Train Epoch: 45 2000\tLoss: 0.0009\n","Train Epoch: 45 3000\tLoss: 0.0009\n","Train Epoch: 45 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 46 0\tLoss: 0.0010\n","Train Epoch: 46 1000\tLoss: 0.0009\n","Train Epoch: 46 2000\tLoss: 0.0009\n","Train Epoch: 46 3000\tLoss: 0.0009\n","Train Epoch: 46 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 47 0\tLoss: 0.0010\n","Train Epoch: 47 1000\tLoss: 0.0009\n","Train Epoch: 47 2000\tLoss: 0.0009\n","Train Epoch: 47 3000\tLoss: 0.0009\n","Train Epoch: 47 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 48 0\tLoss: 0.0010\n","Train Epoch: 48 1000\tLoss: 0.0009\n","Train Epoch: 48 2000\tLoss: 0.0009\n","Train Epoch: 48 3000\tLoss: 0.0009\n","Train Epoch: 48 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 49 0\tLoss: 0.0010\n","Train Epoch: 49 1000\tLoss: 0.0009\n","Train Epoch: 49 2000\tLoss: 0.0009\n","Train Epoch: 49 3000\tLoss: 0.0009\n","Train Epoch: 49 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 50 0\tLoss: 0.0010\n","Train Epoch: 50 1000\tLoss: 0.0009\n","Train Epoch: 50 2000\tLoss: 0.0009\n","Train Epoch: 50 3000\tLoss: 0.0009\n","Train Epoch: 50 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 51 0\tLoss: 0.0010\n","Train Epoch: 51 1000\tLoss: 0.0009\n","Train Epoch: 51 2000\tLoss: 0.0009\n","Train Epoch: 51 3000\tLoss: 0.0009\n","Train Epoch: 51 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 52 0\tLoss: 0.0010\n","Train Epoch: 52 1000\tLoss: 0.0009\n","Train Epoch: 52 2000\tLoss: 0.0009\n","Train Epoch: 52 3000\tLoss: 0.0009\n","Train Epoch: 52 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 53 0\tLoss: 0.0010\n","Train Epoch: 53 1000\tLoss: 0.0009\n","Train Epoch: 53 2000\tLoss: 0.0009\n","Train Epoch: 53 3000\tLoss: 0.0009\n","Train Epoch: 53 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 54 0\tLoss: 0.0010\n","Train Epoch: 54 1000\tLoss: 0.0009\n","Train Epoch: 54 2000\tLoss: 0.0009\n","Train Epoch: 54 3000\tLoss: 0.0009\n","Train Epoch: 54 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 55 0\tLoss: 0.0010\n","Train Epoch: 55 1000\tLoss: 0.0009\n","Train Epoch: 55 2000\tLoss: 0.0009\n","Train Epoch: 55 3000\tLoss: 0.0009\n","Train Epoch: 55 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 56 0\tLoss: 0.0010\n","Train Epoch: 56 1000\tLoss: 0.0009\n","Train Epoch: 56 2000\tLoss: 0.0009\n","Train Epoch: 56 3000\tLoss: 0.0009\n","Train Epoch: 56 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 57 0\tLoss: 0.0010\n","Train Epoch: 57 1000\tLoss: 0.0009\n","Train Epoch: 57 2000\tLoss: 0.0009\n","Train Epoch: 57 3000\tLoss: 0.0009\n","Train Epoch: 57 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 58 0\tLoss: 0.0010\n","Train Epoch: 58 1000\tLoss: 0.0009\n","Train Epoch: 58 2000\tLoss: 0.0009\n","Train Epoch: 58 3000\tLoss: 0.0009\n","Train Epoch: 58 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 59 0\tLoss: 0.0010\n","Train Epoch: 59 1000\tLoss: 0.0009\n","Train Epoch: 59 2000\tLoss: 0.0009\n","Train Epoch: 59 3000\tLoss: 0.0009\n","Train Epoch: 59 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 60 0\tLoss: 0.0010\n","Train Epoch: 60 1000\tLoss: 0.0009\n","Train Epoch: 60 2000\tLoss: 0.0009\n","Train Epoch: 60 3000\tLoss: 0.0009\n","Train Epoch: 60 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 61 0\tLoss: 0.0010\n","Train Epoch: 61 1000\tLoss: 0.0009\n","Train Epoch: 61 2000\tLoss: 0.0009\n","Train Epoch: 61 3000\tLoss: 0.0009\n","Train Epoch: 61 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 62 0\tLoss: 0.0010\n","Train Epoch: 62 1000\tLoss: 0.0009\n","Train Epoch: 62 2000\tLoss: 0.0009\n","Train Epoch: 62 3000\tLoss: 0.0009\n","Train Epoch: 62 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 63 0\tLoss: 0.0010\n","Train Epoch: 63 1000\tLoss: 0.0009\n","Train Epoch: 63 2000\tLoss: 0.0009\n","Train Epoch: 63 3000\tLoss: 0.0009\n","Train Epoch: 63 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 64 0\tLoss: 0.0010\n","Train Epoch: 64 1000\tLoss: 0.0009\n","Train Epoch: 64 2000\tLoss: 0.0009\n","Train Epoch: 64 3000\tLoss: 0.0009\n","Train Epoch: 64 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 65 0\tLoss: 0.0010\n","Train Epoch: 65 1000\tLoss: 0.0009\n","Train Epoch: 65 2000\tLoss: 0.0009\n","Train Epoch: 65 3000\tLoss: 0.0009\n","Train Epoch: 65 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 66 0\tLoss: 0.0010\n","Train Epoch: 66 1000\tLoss: 0.0009\n","Train Epoch: 66 2000\tLoss: 0.0009\n","Train Epoch: 66 3000\tLoss: 0.0009\n","Train Epoch: 66 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 67 0\tLoss: 0.0010\n","Train Epoch: 67 1000\tLoss: 0.0009\n","Train Epoch: 67 2000\tLoss: 0.0009\n","Train Epoch: 67 3000\tLoss: 0.0009\n","Train Epoch: 67 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 68 0\tLoss: 0.0010\n","Train Epoch: 68 1000\tLoss: 0.0009\n","Train Epoch: 68 2000\tLoss: 0.0009\n","Train Epoch: 68 3000\tLoss: 0.0009\n","Train Epoch: 68 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 69 0\tLoss: 0.0010\n","Train Epoch: 69 1000\tLoss: 0.0009\n","Train Epoch: 69 2000\tLoss: 0.0009\n","Train Epoch: 69 3000\tLoss: 0.0009\n","Train Epoch: 69 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 70 0\tLoss: 0.0010\n","Train Epoch: 70 1000\tLoss: 0.0009\n","Train Epoch: 70 2000\tLoss: 0.0009\n","Train Epoch: 70 3000\tLoss: 0.0009\n","Train Epoch: 70 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 71 0\tLoss: 0.0010\n","Train Epoch: 71 1000\tLoss: 0.0009\n","Train Epoch: 71 2000\tLoss: 0.0009\n","Train Epoch: 71 3000\tLoss: 0.0009\n","Train Epoch: 71 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 72 0\tLoss: 0.0010\n","Train Epoch: 72 1000\tLoss: 0.0009\n","Train Epoch: 72 2000\tLoss: 0.0009\n","Train Epoch: 72 3000\tLoss: 0.0009\n","Train Epoch: 72 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 73 0\tLoss: 0.0010\n","Train Epoch: 73 1000\tLoss: 0.0009\n","Train Epoch: 73 2000\tLoss: 0.0010\n","Train Epoch: 73 3000\tLoss: 0.0009\n","Train Epoch: 73 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 74 0\tLoss: 0.0010\n","Train Epoch: 74 1000\tLoss: 0.0009\n","Train Epoch: 74 2000\tLoss: 0.0010\n","Train Epoch: 74 3000\tLoss: 0.0009\n","Train Epoch: 74 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 75 0\tLoss: 0.0010\n","Train Epoch: 75 1000\tLoss: 0.0009\n","Train Epoch: 75 2000\tLoss: 0.0010\n","Train Epoch: 75 3000\tLoss: 0.0009\n","Train Epoch: 75 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 76 0\tLoss: 0.0010\n","Train Epoch: 76 1000\tLoss: 0.0009\n","Train Epoch: 76 2000\tLoss: 0.0010\n","Train Epoch: 76 3000\tLoss: 0.0009\n","Train Epoch: 76 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 77 0\tLoss: 0.0010\n","Train Epoch: 77 1000\tLoss: 0.0009\n","Train Epoch: 77 2000\tLoss: 0.0010\n","Train Epoch: 77 3000\tLoss: 0.0009\n","Train Epoch: 77 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 78 0\tLoss: 0.0010\n","Train Epoch: 78 1000\tLoss: 0.0009\n","Train Epoch: 78 2000\tLoss: 0.0010\n","Train Epoch: 78 3000\tLoss: 0.0009\n","Train Epoch: 78 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 79 0\tLoss: 0.0010\n","Train Epoch: 79 1000\tLoss: 0.0009\n","Train Epoch: 79 2000\tLoss: 0.0010\n","Train Epoch: 79 3000\tLoss: 0.0009\n","Train Epoch: 79 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 80 0\tLoss: 0.0010\n","Train Epoch: 80 1000\tLoss: 0.0009\n","Train Epoch: 80 2000\tLoss: 0.0010\n","Train Epoch: 80 3000\tLoss: 0.0009\n","Train Epoch: 80 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 81 0\tLoss: 0.0010\n","Train Epoch: 81 1000\tLoss: 0.0009\n","Train Epoch: 81 2000\tLoss: 0.0010\n","Train Epoch: 81 3000\tLoss: 0.0009\n","Train Epoch: 81 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 82 0\tLoss: 0.0010\n","Train Epoch: 82 1000\tLoss: 0.0009\n","Train Epoch: 82 2000\tLoss: 0.0010\n","Train Epoch: 82 3000\tLoss: 0.0009\n","Train Epoch: 82 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 83 0\tLoss: 0.0010\n","Train Epoch: 83 1000\tLoss: 0.0009\n","Train Epoch: 83 2000\tLoss: 0.0010\n","Train Epoch: 83 3000\tLoss: 0.0009\n","Train Epoch: 83 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 84 0\tLoss: 0.0010\n","Train Epoch: 84 1000\tLoss: 0.0009\n","Train Epoch: 84 2000\tLoss: 0.0010\n","Train Epoch: 84 3000\tLoss: 0.0009\n","Train Epoch: 84 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 85 0\tLoss: 0.0010\n","Train Epoch: 85 1000\tLoss: 0.0009\n","Train Epoch: 85 2000\tLoss: 0.0010\n","Train Epoch: 85 3000\tLoss: 0.0009\n","Train Epoch: 85 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 86 0\tLoss: 0.0010\n","Train Epoch: 86 1000\tLoss: 0.0009\n","Train Epoch: 86 2000\tLoss: 0.0010\n","Train Epoch: 86 3000\tLoss: 0.0009\n","Train Epoch: 86 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 87 0\tLoss: 0.0010\n","Train Epoch: 87 1000\tLoss: 0.0009\n","Train Epoch: 87 2000\tLoss: 0.0010\n","Train Epoch: 87 3000\tLoss: 0.0009\n","Train Epoch: 87 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 88 0\tLoss: 0.0010\n","Train Epoch: 88 1000\tLoss: 0.0009\n","Train Epoch: 88 2000\tLoss: 0.0010\n","Train Epoch: 88 3000\tLoss: 0.0009\n","Train Epoch: 88 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 89 0\tLoss: 0.0010\n","Train Epoch: 89 1000\tLoss: 0.0009\n","Train Epoch: 89 2000\tLoss: 0.0010\n","Train Epoch: 89 3000\tLoss: 0.0009\n","Train Epoch: 89 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 90 0\tLoss: 0.0010\n","Train Epoch: 90 1000\tLoss: 0.0009\n","Train Epoch: 90 2000\tLoss: 0.0010\n","Train Epoch: 90 3000\tLoss: 0.0009\n","Train Epoch: 90 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 91 0\tLoss: 0.0010\n","Train Epoch: 91 1000\tLoss: 0.0009\n","Train Epoch: 91 2000\tLoss: 0.0010\n","Train Epoch: 91 3000\tLoss: 0.0009\n","Train Epoch: 91 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 92 0\tLoss: 0.0010\n","Train Epoch: 92 1000\tLoss: 0.0009\n","Train Epoch: 92 2000\tLoss: 0.0010\n","Train Epoch: 92 3000\tLoss: 0.0009\n","Train Epoch: 92 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 93 0\tLoss: 0.0010\n","Train Epoch: 93 1000\tLoss: 0.0009\n","Train Epoch: 93 2000\tLoss: 0.0010\n","Train Epoch: 93 3000\tLoss: 0.0009\n","Train Epoch: 93 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 94 0\tLoss: 0.0010\n","Train Epoch: 94 1000\tLoss: 0.0009\n","Train Epoch: 94 2000\tLoss: 0.0010\n","Train Epoch: 94 3000\tLoss: 0.0009\n","Train Epoch: 94 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 95 0\tLoss: 0.0010\n","Train Epoch: 95 1000\tLoss: 0.0009\n","Train Epoch: 95 2000\tLoss: 0.0010\n","Train Epoch: 95 3000\tLoss: 0.0009\n","Train Epoch: 95 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 96 0\tLoss: 0.0010\n","Train Epoch: 96 1000\tLoss: 0.0009\n","Train Epoch: 96 2000\tLoss: 0.0010\n","Train Epoch: 96 3000\tLoss: 0.0009\n","Train Epoch: 96 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 97 0\tLoss: 0.0010\n","Train Epoch: 97 1000\tLoss: 0.0009\n","Train Epoch: 97 2000\tLoss: 0.0010\n","Train Epoch: 97 3000\tLoss: 0.0009\n","Train Epoch: 97 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 98 0\tLoss: 0.0010\n","Train Epoch: 98 1000\tLoss: 0.0009\n","Train Epoch: 98 2000\tLoss: 0.0010\n","Train Epoch: 98 3000\tLoss: 0.0009\n","Train Epoch: 98 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 99 0\tLoss: 0.0010\n","Train Epoch: 99 1000\tLoss: 0.0009\n","Train Epoch: 99 2000\tLoss: 0.0010\n","Train Epoch: 99 3000\tLoss: 0.0009\n","Train Epoch: 99 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 100 0\tLoss: 0.0010\n","Train Epoch: 100 1000\tLoss: 0.0009\n","Train Epoch: 100 2000\tLoss: 0.0010\n","Train Epoch: 100 3000\tLoss: 0.0009\n","Train Epoch: 100 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0009, Accuracy: 500/500 (100.00%)\n","\n"]}],"source":["## run experiment \n","nepochs = 100\n","lr = 0.1\n","\n","print('lr=', lr)\n","for epoch in range(1, nepochs + 1):\n","    train(epoch)\n","    print('---------------------------------------------')\n","    test()\n"," \n","# everytime rerun this cell, please re initialize your network, and re run the train test function "]},{"cell_type":"markdown","metadata":{"id":"wM3QoS2buZe7"},"source":["## Exercise 3 (6%) \n","For this experiment, try the following learning rate (lr=0.0001, 0.001, 0.01, 0.1). What do you observed? <br><br>\n","For example, at lr=0.001, test acc reach 100% at epoch xx... At lr=0.001, test acc reach 100% at epoch xx. As lr increases / decreases, what happen?\n"]},{"cell_type":"markdown","metadata":{"id":"yH5tn9G1ugD1"},"source":["### Your answer here \n","When lr=0.0001, testing accuracy reach 100% at epoch `183`. <br>\n","When lr=0.001, testing accuracy reach 100% at epoch `16`. <br>\n","When lr=0.01, testing accuracy reach 100% at epoch `2`. <br>\n","When lr=0.1, testing accuracy reach 100% at epoch `1`. <br> <br>\n","\n","\n","To conclude, when lr increases, `the epoch or the number of training decreases`."]},{"cell_type":"code","execution_count":110,"metadata":{"id":"NGFk29Y0Y38E","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1667928290738,"user_tz":-480,"elapsed":32097,"user":{"displayName":"CHIA HSING CHONG","userId":"13471313610066442776"}},"outputId":"ab3d96db-7c0c-49c4-987b-f776a9f26192"},"outputs":[{"output_type":"stream","name":"stdout","text":["lr= 0.001\n","Train Epoch: 1 0\tLoss: 1.8077\n","Train Epoch: 1 1000\tLoss: 1.2709\n","Train Epoch: 1 2000\tLoss: 1.3170\n","Train Epoch: 1 3000\tLoss: 1.3284\n","Train Epoch: 1 4000\tLoss: 1.2323\n","---------------------------------------------\n","\n","Test set: Average loss: 0.9848, Accuracy: 0/500 (0.00%)\n","\n","Train Epoch: 2 0\tLoss: 1.0818\n","Train Epoch: 2 1000\tLoss: 1.0590\n","Train Epoch: 2 2000\tLoss: 1.0610\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"]},{"output_type":"stream","name":"stdout","text":["Train Epoch: 2 3000\tLoss: 1.0364\n","Train Epoch: 2 4000\tLoss: 1.0139\n","---------------------------------------------\n","\n","Test set: Average loss: 0.9751, Accuracy: 0/500 (0.00%)\n","\n","Train Epoch: 3 0\tLoss: 0.9246\n","Train Epoch: 3 1000\tLoss: 1.0164\n","Train Epoch: 3 2000\tLoss: 0.9913\n","Train Epoch: 3 3000\tLoss: 0.9581\n","Train Epoch: 3 4000\tLoss: 0.9459\n","---------------------------------------------\n","\n","Test set: Average loss: 0.9692, Accuracy: 0/500 (0.00%)\n","\n","Train Epoch: 4 0\tLoss: 0.8649\n","Train Epoch: 4 1000\tLoss: 0.9808\n","Train Epoch: 4 2000\tLoss: 0.9511\n","Train Epoch: 4 3000\tLoss: 0.9278\n","Train Epoch: 4 4000\tLoss: 0.9111\n","---------------------------------------------\n","\n","Test set: Average loss: 0.9532, Accuracy: 0/500 (0.00%)\n","\n","Train Epoch: 5 0\tLoss: 0.8246\n","Train Epoch: 5 1000\tLoss: 0.9414\n","Train Epoch: 5 2000\tLoss: 0.9157\n","Train Epoch: 5 3000\tLoss: 0.9080\n","Train Epoch: 5 4000\tLoss: 0.8815\n","---------------------------------------------\n","\n","Test set: Average loss: 0.9294, Accuracy: 0/500 (0.00%)\n","\n","Train Epoch: 6 0\tLoss: 0.7853\n","Train Epoch: 6 1000\tLoss: 0.8993\n","Train Epoch: 6 2000\tLoss: 0.8795\n","Train Epoch: 6 3000\tLoss: 0.8871\n","Train Epoch: 6 4000\tLoss: 0.8491\n","---------------------------------------------\n","\n","Test set: Average loss: 0.8993, Accuracy: 0/500 (0.00%)\n","\n","Train Epoch: 7 0\tLoss: 0.7421\n","Train Epoch: 7 1000\tLoss: 0.8550\n","Train Epoch: 7 2000\tLoss: 0.8408\n","Train Epoch: 7 3000\tLoss: 0.8616\n","Train Epoch: 7 4000\tLoss: 0.8118\n","---------------------------------------------\n","\n","Test set: Average loss: 0.8641, Accuracy: 0/500 (0.00%)\n","\n","Train Epoch: 8 0\tLoss: 0.6948\n","Train Epoch: 8 1000\tLoss: 0.8094\n","Train Epoch: 8 2000\tLoss: 0.7996\n","Train Epoch: 8 3000\tLoss: 0.8309\n","Train Epoch: 8 4000\tLoss: 0.7704\n","---------------------------------------------\n","\n","Test set: Average loss: 0.8251, Accuracy: 0/500 (0.00%)\n","\n","Train Epoch: 9 0\tLoss: 0.6453\n","Train Epoch: 9 1000\tLoss: 0.7634\n","Train Epoch: 9 2000\tLoss: 0.7568\n","Train Epoch: 9 3000\tLoss: 0.7956\n","Train Epoch: 9 4000\tLoss: 0.7265\n","---------------------------------------------\n","\n","Test set: Average loss: 0.7833, Accuracy: 0/500 (0.00%)\n","\n","Train Epoch: 10 0\tLoss: 0.5959\n","Train Epoch: 10 1000\tLoss: 0.7179\n","Train Epoch: 10 2000\tLoss: 0.7129\n","Train Epoch: 10 3000\tLoss: 0.7564\n","Train Epoch: 10 4000\tLoss: 0.6815\n","---------------------------------------------\n","\n","Test set: Average loss: 0.7385, Accuracy: 0/500 (0.00%)\n","\n","Train Epoch: 11 0\tLoss: 0.5483\n","Train Epoch: 11 1000\tLoss: 0.6724\n","Train Epoch: 11 2000\tLoss: 0.6676\n","Train Epoch: 11 3000\tLoss: 0.7130\n","Train Epoch: 11 4000\tLoss: 0.6354\n","---------------------------------------------\n","\n","Test set: Average loss: 0.6887, Accuracy: 87/500 (17.40%)\n","\n","Train Epoch: 12 0\tLoss: 0.5027\n","Train Epoch: 12 1000\tLoss: 0.6235\n","Train Epoch: 12 2000\tLoss: 0.6175\n","Train Epoch: 12 3000\tLoss: 0.6629\n","Train Epoch: 12 4000\tLoss: 0.5860\n","---------------------------------------------\n","\n","Test set: Average loss: 0.6282, Accuracy: 153/500 (30.60%)\n","\n","Train Epoch: 13 0\tLoss: 0.4570\n","Train Epoch: 13 1000\tLoss: 0.5633\n","Train Epoch: 13 2000\tLoss: 0.5550\n","Train Epoch: 13 3000\tLoss: 0.5999\n","Train Epoch: 13 4000\tLoss: 0.5275\n","---------------------------------------------\n","\n","Test set: Average loss: 0.5469, Accuracy: 188/500 (37.60%)\n","\n","Train Epoch: 14 0\tLoss: 0.4061\n","Train Epoch: 14 1000\tLoss: 0.4795\n","Train Epoch: 14 2000\tLoss: 0.4695\n","Train Epoch: 14 3000\tLoss: 0.5150\n","Train Epoch: 14 4000\tLoss: 0.4526\n","---------------------------------------------\n","\n","Test set: Average loss: 0.4395, Accuracy: 208/500 (41.60%)\n","\n","Train Epoch: 15 0\tLoss: 0.3446\n","Train Epoch: 15 1000\tLoss: 0.3719\n","Train Epoch: 15 2000\tLoss: 0.3645\n","Train Epoch: 15 3000\tLoss: 0.4090\n","Train Epoch: 15 4000\tLoss: 0.3623\n","---------------------------------------------\n","\n","Test set: Average loss: 0.3245, Accuracy: 308/500 (61.60%)\n","\n","Train Epoch: 16 0\tLoss: 0.2741\n","Train Epoch: 16 1000\tLoss: 0.2681\n","Train Epoch: 16 2000\tLoss: 0.2638\n","Train Epoch: 16 3000\tLoss: 0.3000\n","Train Epoch: 16 4000\tLoss: 0.2705\n","---------------------------------------------\n","\n","Test set: Average loss: 0.2257, Accuracy: 380/500 (76.00%)\n","\n","Train Epoch: 17 0\tLoss: 0.2040\n","Train Epoch: 17 1000\tLoss: 0.1872\n","Train Epoch: 17 2000\tLoss: 0.1828\n","Train Epoch: 17 3000\tLoss: 0.2066\n","Train Epoch: 17 4000\tLoss: 0.1908\n","---------------------------------------------\n","\n","Test set: Average loss: 0.1507, Accuracy: 380/500 (76.00%)\n","\n","Train Epoch: 18 0\tLoss: 0.1431\n","Train Epoch: 18 1000\tLoss: 0.1281\n","Train Epoch: 18 2000\tLoss: 0.1224\n","Train Epoch: 18 3000\tLoss: 0.1360\n","Train Epoch: 18 4000\tLoss: 0.1293\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0979, Accuracy: 476/500 (95.20%)\n","\n","Train Epoch: 19 0\tLoss: 0.0960\n","Train Epoch: 19 1000\tLoss: 0.0860\n","Train Epoch: 19 2000\tLoss: 0.0796\n","Train Epoch: 19 3000\tLoss: 0.0867\n","Train Epoch: 19 4000\tLoss: 0.0853\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0625, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 20 0\tLoss: 0.0623\n","Train Epoch: 20 1000\tLoss: 0.0569\n","Train Epoch: 20 2000\tLoss: 0.0506\n","Train Epoch: 20 3000\tLoss: 0.0541\n","Train Epoch: 20 4000\tLoss: 0.0554\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0395, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 21 0\tLoss: 0.0395\n","Train Epoch: 21 1000\tLoss: 0.0374\n","Train Epoch: 21 2000\tLoss: 0.0316\n","Train Epoch: 21 3000\tLoss: 0.0333\n","Train Epoch: 21 4000\tLoss: 0.0357\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0249, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 22 0\tLoss: 0.0247\n","Train Epoch: 22 1000\tLoss: 0.0246\n","Train Epoch: 22 2000\tLoss: 0.0195\n","Train Epoch: 22 3000\tLoss: 0.0203\n","Train Epoch: 22 4000\tLoss: 0.0230\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0158, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 23 0\tLoss: 0.0153\n","Train Epoch: 23 1000\tLoss: 0.0163\n","Train Epoch: 23 2000\tLoss: 0.0120\n","Train Epoch: 23 3000\tLoss: 0.0124\n","Train Epoch: 23 4000\tLoss: 0.0149\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0102, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 24 0\tLoss: 0.0096\n","Train Epoch: 24 1000\tLoss: 0.0111\n","Train Epoch: 24 2000\tLoss: 0.0073\n","Train Epoch: 24 3000\tLoss: 0.0076\n","Train Epoch: 24 4000\tLoss: 0.0098\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0067, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 25 0\tLoss: 0.0061\n","Train Epoch: 25 1000\tLoss: 0.0077\n","Train Epoch: 25 2000\tLoss: 0.0045\n","Train Epoch: 25 3000\tLoss: 0.0048\n","Train Epoch: 25 4000\tLoss: 0.0065\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0045, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 26 0\tLoss: 0.0040\n","Train Epoch: 26 1000\tLoss: 0.0056\n","Train Epoch: 26 2000\tLoss: 0.0029\n","Train Epoch: 26 3000\tLoss: 0.0032\n","Train Epoch: 26 4000\tLoss: 0.0045\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0032, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 27 0\tLoss: 0.0028\n","Train Epoch: 27 1000\tLoss: 0.0043\n","Train Epoch: 27 2000\tLoss: 0.0019\n","Train Epoch: 27 3000\tLoss: 0.0022\n","Train Epoch: 27 4000\tLoss: 0.0032\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0024, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 28 0\tLoss: 0.0021\n","Train Epoch: 28 1000\tLoss: 0.0035\n","Train Epoch: 28 2000\tLoss: 0.0014\n","Train Epoch: 28 3000\tLoss: 0.0017\n","Train Epoch: 28 4000\tLoss: 0.0024\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0019, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 29 0\tLoss: 0.0017\n","Train Epoch: 29 1000\tLoss: 0.0030\n","Train Epoch: 29 2000\tLoss: 0.0011\n","Train Epoch: 29 3000\tLoss: 0.0015\n","Train Epoch: 29 4000\tLoss: 0.0019\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0016, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 30 0\tLoss: 0.0016\n","Train Epoch: 30 1000\tLoss: 0.0026\n","Train Epoch: 30 2000\tLoss: 0.0009\n","Train Epoch: 30 3000\tLoss: 0.0014\n","Train Epoch: 30 4000\tLoss: 0.0015\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0014, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 31 0\tLoss: 0.0015\n","Train Epoch: 31 1000\tLoss: 0.0024\n","Train Epoch: 31 2000\tLoss: 0.0008\n","Train Epoch: 31 3000\tLoss: 0.0013\n","Train Epoch: 31 4000\tLoss: 0.0013\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0013, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 32 0\tLoss: 0.0014\n","Train Epoch: 32 1000\tLoss: 0.0023\n","Train Epoch: 32 2000\tLoss: 0.0008\n","Train Epoch: 32 3000\tLoss: 0.0013\n","Train Epoch: 32 4000\tLoss: 0.0012\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0012, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 33 0\tLoss: 0.0014\n","Train Epoch: 33 1000\tLoss: 0.0022\n","Train Epoch: 33 2000\tLoss: 0.0008\n","Train Epoch: 33 3000\tLoss: 0.0013\n","Train Epoch: 33 4000\tLoss: 0.0011\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0012, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 34 0\tLoss: 0.0015\n","Train Epoch: 34 1000\tLoss: 0.0021\n","Train Epoch: 34 2000\tLoss: 0.0008\n","Train Epoch: 34 3000\tLoss: 0.0013\n","Train Epoch: 34 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0012, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 35 0\tLoss: 0.0015\n","Train Epoch: 35 1000\tLoss: 0.0021\n","Train Epoch: 35 2000\tLoss: 0.0008\n","Train Epoch: 35 3000\tLoss: 0.0014\n","Train Epoch: 35 4000\tLoss: 0.0010\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 36 0\tLoss: 0.0015\n","Train Epoch: 36 1000\tLoss: 0.0020\n","Train Epoch: 36 2000\tLoss: 0.0008\n","Train Epoch: 36 3000\tLoss: 0.0014\n","Train Epoch: 36 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 37 0\tLoss: 0.0015\n","Train Epoch: 37 1000\tLoss: 0.0020\n","Train Epoch: 37 2000\tLoss: 0.0008\n","Train Epoch: 37 3000\tLoss: 0.0014\n","Train Epoch: 37 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 38 0\tLoss: 0.0015\n","Train Epoch: 38 1000\tLoss: 0.0020\n","Train Epoch: 38 2000\tLoss: 0.0008\n","Train Epoch: 38 3000\tLoss: 0.0014\n","Train Epoch: 38 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 39 0\tLoss: 0.0015\n","Train Epoch: 39 1000\tLoss: 0.0020\n","Train Epoch: 39 2000\tLoss: 0.0009\n","Train Epoch: 39 3000\tLoss: 0.0014\n","Train Epoch: 39 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 40 0\tLoss: 0.0016\n","Train Epoch: 40 1000\tLoss: 0.0020\n","Train Epoch: 40 2000\tLoss: 0.0009\n","Train Epoch: 40 3000\tLoss: 0.0014\n","Train Epoch: 40 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 41 0\tLoss: 0.0016\n","Train Epoch: 41 1000\tLoss: 0.0020\n","Train Epoch: 41 2000\tLoss: 0.0009\n","Train Epoch: 41 3000\tLoss: 0.0015\n","Train Epoch: 41 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 42 0\tLoss: 0.0016\n","Train Epoch: 42 1000\tLoss: 0.0020\n","Train Epoch: 42 2000\tLoss: 0.0009\n","Train Epoch: 42 3000\tLoss: 0.0015\n","Train Epoch: 42 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 43 0\tLoss: 0.0016\n","Train Epoch: 43 1000\tLoss: 0.0020\n","Train Epoch: 43 2000\tLoss: 0.0009\n","Train Epoch: 43 3000\tLoss: 0.0015\n","Train Epoch: 43 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 44 0\tLoss: 0.0016\n","Train Epoch: 44 1000\tLoss: 0.0020\n","Train Epoch: 44 2000\tLoss: 0.0009\n","Train Epoch: 44 3000\tLoss: 0.0015\n","Train Epoch: 44 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 45 0\tLoss: 0.0016\n","Train Epoch: 45 1000\tLoss: 0.0020\n","Train Epoch: 45 2000\tLoss: 0.0009\n","Train Epoch: 45 3000\tLoss: 0.0015\n","Train Epoch: 45 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 46 0\tLoss: 0.0016\n","Train Epoch: 46 1000\tLoss: 0.0019\n","Train Epoch: 46 2000\tLoss: 0.0009\n","Train Epoch: 46 3000\tLoss: 0.0015\n","Train Epoch: 46 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 47 0\tLoss: 0.0016\n","Train Epoch: 47 1000\tLoss: 0.0019\n","Train Epoch: 47 2000\tLoss: 0.0009\n","Train Epoch: 47 3000\tLoss: 0.0015\n","Train Epoch: 47 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 48 0\tLoss: 0.0016\n","Train Epoch: 48 1000\tLoss: 0.0019\n","Train Epoch: 48 2000\tLoss: 0.0009\n","Train Epoch: 48 3000\tLoss: 0.0015\n","Train Epoch: 48 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 49 0\tLoss: 0.0016\n","Train Epoch: 49 1000\tLoss: 0.0019\n","Train Epoch: 49 2000\tLoss: 0.0009\n","Train Epoch: 49 3000\tLoss: 0.0015\n","Train Epoch: 49 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 50 0\tLoss: 0.0016\n","Train Epoch: 50 1000\tLoss: 0.0019\n","Train Epoch: 50 2000\tLoss: 0.0009\n","Train Epoch: 50 3000\tLoss: 0.0015\n","Train Epoch: 50 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 51 0\tLoss: 0.0016\n","Train Epoch: 51 1000\tLoss: 0.0019\n","Train Epoch: 51 2000\tLoss: 0.0009\n","Train Epoch: 51 3000\tLoss: 0.0015\n","Train Epoch: 51 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 52 0\tLoss: 0.0016\n","Train Epoch: 52 1000\tLoss: 0.0019\n","Train Epoch: 52 2000\tLoss: 0.0009\n","Train Epoch: 52 3000\tLoss: 0.0015\n","Train Epoch: 52 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 53 0\tLoss: 0.0016\n","Train Epoch: 53 1000\tLoss: 0.0019\n","Train Epoch: 53 2000\tLoss: 0.0009\n","Train Epoch: 53 3000\tLoss: 0.0015\n","Train Epoch: 53 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 54 0\tLoss: 0.0016\n","Train Epoch: 54 1000\tLoss: 0.0019\n","Train Epoch: 54 2000\tLoss: 0.0009\n","Train Epoch: 54 3000\tLoss: 0.0015\n","Train Epoch: 54 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 55 0\tLoss: 0.0016\n","Train Epoch: 55 1000\tLoss: 0.0019\n","Train Epoch: 55 2000\tLoss: 0.0009\n","Train Epoch: 55 3000\tLoss: 0.0015\n","Train Epoch: 55 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 56 0\tLoss: 0.0016\n","Train Epoch: 56 1000\tLoss: 0.0019\n","Train Epoch: 56 2000\tLoss: 0.0009\n","Train Epoch: 56 3000\tLoss: 0.0015\n","Train Epoch: 56 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 57 0\tLoss: 0.0016\n","Train Epoch: 57 1000\tLoss: 0.0019\n","Train Epoch: 57 2000\tLoss: 0.0009\n","Train Epoch: 57 3000\tLoss: 0.0015\n","Train Epoch: 57 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 58 0\tLoss: 0.0016\n","Train Epoch: 58 1000\tLoss: 0.0019\n","Train Epoch: 58 2000\tLoss: 0.0009\n","Train Epoch: 58 3000\tLoss: 0.0015\n","Train Epoch: 58 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 59 0\tLoss: 0.0016\n","Train Epoch: 59 1000\tLoss: 0.0019\n","Train Epoch: 59 2000\tLoss: 0.0009\n","Train Epoch: 59 3000\tLoss: 0.0015\n","Train Epoch: 59 4000\tLoss: 0.0009\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 60 0\tLoss: 0.0016\n","Train Epoch: 60 1000\tLoss: 0.0019\n","Train Epoch: 60 2000\tLoss: 0.0009\n","Train Epoch: 60 3000\tLoss: 0.0015\n","Train Epoch: 60 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 61 0\tLoss: 0.0016\n","Train Epoch: 61 1000\tLoss: 0.0019\n","Train Epoch: 61 2000\tLoss: 0.0009\n","Train Epoch: 61 3000\tLoss: 0.0015\n","Train Epoch: 61 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 62 0\tLoss: 0.0016\n","Train Epoch: 62 1000\tLoss: 0.0019\n","Train Epoch: 62 2000\tLoss: 0.0009\n","Train Epoch: 62 3000\tLoss: 0.0015\n","Train Epoch: 62 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 63 0\tLoss: 0.0016\n","Train Epoch: 63 1000\tLoss: 0.0019\n","Train Epoch: 63 2000\tLoss: 0.0009\n","Train Epoch: 63 3000\tLoss: 0.0015\n","Train Epoch: 63 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 64 0\tLoss: 0.0016\n","Train Epoch: 64 1000\tLoss: 0.0019\n","Train Epoch: 64 2000\tLoss: 0.0009\n","Train Epoch: 64 3000\tLoss: 0.0015\n","Train Epoch: 64 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 65 0\tLoss: 0.0016\n","Train Epoch: 65 1000\tLoss: 0.0019\n","Train Epoch: 65 2000\tLoss: 0.0009\n","Train Epoch: 65 3000\tLoss: 0.0015\n","Train Epoch: 65 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 66 0\tLoss: 0.0016\n","Train Epoch: 66 1000\tLoss: 0.0019\n","Train Epoch: 66 2000\tLoss: 0.0009\n","Train Epoch: 66 3000\tLoss: 0.0015\n","Train Epoch: 66 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 67 0\tLoss: 0.0016\n","Train Epoch: 67 1000\tLoss: 0.0019\n","Train Epoch: 67 2000\tLoss: 0.0009\n","Train Epoch: 67 3000\tLoss: 0.0015\n","Train Epoch: 67 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 68 0\tLoss: 0.0016\n","Train Epoch: 68 1000\tLoss: 0.0019\n","Train Epoch: 68 2000\tLoss: 0.0009\n","Train Epoch: 68 3000\tLoss: 0.0015\n","Train Epoch: 68 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 69 0\tLoss: 0.0016\n","Train Epoch: 69 1000\tLoss: 0.0019\n","Train Epoch: 69 2000\tLoss: 0.0009\n","Train Epoch: 69 3000\tLoss: 0.0015\n","Train Epoch: 69 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 70 0\tLoss: 0.0016\n","Train Epoch: 70 1000\tLoss: 0.0019\n","Train Epoch: 70 2000\tLoss: 0.0009\n","Train Epoch: 70 3000\tLoss: 0.0015\n","Train Epoch: 70 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 71 0\tLoss: 0.0016\n","Train Epoch: 71 1000\tLoss: 0.0019\n","Train Epoch: 71 2000\tLoss: 0.0009\n","Train Epoch: 71 3000\tLoss: 0.0015\n","Train Epoch: 71 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 72 0\tLoss: 0.0016\n","Train Epoch: 72 1000\tLoss: 0.0019\n","Train Epoch: 72 2000\tLoss: 0.0009\n","Train Epoch: 72 3000\tLoss: 0.0015\n","Train Epoch: 72 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 73 0\tLoss: 0.0016\n","Train Epoch: 73 1000\tLoss: 0.0019\n","Train Epoch: 73 2000\tLoss: 0.0009\n","Train Epoch: 73 3000\tLoss: 0.0015\n","Train Epoch: 73 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 74 0\tLoss: 0.0016\n","Train Epoch: 74 1000\tLoss: 0.0019\n","Train Epoch: 74 2000\tLoss: 0.0009\n","Train Epoch: 74 3000\tLoss: 0.0015\n","Train Epoch: 74 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 75 0\tLoss: 0.0016\n","Train Epoch: 75 1000\tLoss: 0.0019\n","Train Epoch: 75 2000\tLoss: 0.0009\n","Train Epoch: 75 3000\tLoss: 0.0015\n","Train Epoch: 75 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 76 0\tLoss: 0.0016\n","Train Epoch: 76 1000\tLoss: 0.0019\n","Train Epoch: 76 2000\tLoss: 0.0009\n","Train Epoch: 76 3000\tLoss: 0.0015\n","Train Epoch: 76 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 77 0\tLoss: 0.0016\n","Train Epoch: 77 1000\tLoss: 0.0019\n","Train Epoch: 77 2000\tLoss: 0.0009\n","Train Epoch: 77 3000\tLoss: 0.0015\n","Train Epoch: 77 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 78 0\tLoss: 0.0016\n","Train Epoch: 78 1000\tLoss: 0.0019\n","Train Epoch: 78 2000\tLoss: 0.0009\n","Train Epoch: 78 3000\tLoss: 0.0015\n","Train Epoch: 78 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 79 0\tLoss: 0.0016\n","Train Epoch: 79 1000\tLoss: 0.0019\n","Train Epoch: 79 2000\tLoss: 0.0009\n","Train Epoch: 79 3000\tLoss: 0.0015\n","Train Epoch: 79 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 80 0\tLoss: 0.0016\n","Train Epoch: 80 1000\tLoss: 0.0019\n","Train Epoch: 80 2000\tLoss: 0.0009\n","Train Epoch: 80 3000\tLoss: 0.0015\n","Train Epoch: 80 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 81 0\tLoss: 0.0016\n","Train Epoch: 81 1000\tLoss: 0.0019\n","Train Epoch: 81 2000\tLoss: 0.0009\n","Train Epoch: 81 3000\tLoss: 0.0015\n","Train Epoch: 81 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 82 0\tLoss: 0.0016\n","Train Epoch: 82 1000\tLoss: 0.0019\n","Train Epoch: 82 2000\tLoss: 0.0009\n","Train Epoch: 82 3000\tLoss: 0.0015\n","Train Epoch: 82 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 83 0\tLoss: 0.0016\n","Train Epoch: 83 1000\tLoss: 0.0019\n","Train Epoch: 83 2000\tLoss: 0.0009\n","Train Epoch: 83 3000\tLoss: 0.0015\n","Train Epoch: 83 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 84 0\tLoss: 0.0016\n","Train Epoch: 84 1000\tLoss: 0.0019\n","Train Epoch: 84 2000\tLoss: 0.0009\n","Train Epoch: 84 3000\tLoss: 0.0015\n","Train Epoch: 84 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 85 0\tLoss: 0.0015\n","Train Epoch: 85 1000\tLoss: 0.0019\n","Train Epoch: 85 2000\tLoss: 0.0009\n","Train Epoch: 85 3000\tLoss: 0.0015\n","Train Epoch: 85 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 86 0\tLoss: 0.0015\n","Train Epoch: 86 1000\tLoss: 0.0019\n","Train Epoch: 86 2000\tLoss: 0.0009\n","Train Epoch: 86 3000\tLoss: 0.0015\n","Train Epoch: 86 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 87 0\tLoss: 0.0015\n","Train Epoch: 87 1000\tLoss: 0.0019\n","Train Epoch: 87 2000\tLoss: 0.0009\n","Train Epoch: 87 3000\tLoss: 0.0015\n","Train Epoch: 87 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 88 0\tLoss: 0.0015\n","Train Epoch: 88 1000\tLoss: 0.0019\n","Train Epoch: 88 2000\tLoss: 0.0009\n","Train Epoch: 88 3000\tLoss: 0.0015\n","Train Epoch: 88 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 89 0\tLoss: 0.0015\n","Train Epoch: 89 1000\tLoss: 0.0019\n","Train Epoch: 89 2000\tLoss: 0.0009\n","Train Epoch: 89 3000\tLoss: 0.0015\n","Train Epoch: 89 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 90 0\tLoss: 0.0015\n","Train Epoch: 90 1000\tLoss: 0.0019\n","Train Epoch: 90 2000\tLoss: 0.0009\n","Train Epoch: 90 3000\tLoss: 0.0015\n","Train Epoch: 90 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 91 0\tLoss: 0.0015\n","Train Epoch: 91 1000\tLoss: 0.0019\n","Train Epoch: 91 2000\tLoss: 0.0009\n","Train Epoch: 91 3000\tLoss: 0.0015\n","Train Epoch: 91 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 92 0\tLoss: 0.0015\n","Train Epoch: 92 1000\tLoss: 0.0019\n","Train Epoch: 92 2000\tLoss: 0.0009\n","Train Epoch: 92 3000\tLoss: 0.0015\n","Train Epoch: 92 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 93 0\tLoss: 0.0015\n","Train Epoch: 93 1000\tLoss: 0.0019\n","Train Epoch: 93 2000\tLoss: 0.0009\n","Train Epoch: 93 3000\tLoss: 0.0015\n","Train Epoch: 93 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 94 0\tLoss: 0.0015\n","Train Epoch: 94 1000\tLoss: 0.0019\n","Train Epoch: 94 2000\tLoss: 0.0009\n","Train Epoch: 94 3000\tLoss: 0.0015\n","Train Epoch: 94 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 95 0\tLoss: 0.0015\n","Train Epoch: 95 1000\tLoss: 0.0019\n","Train Epoch: 95 2000\tLoss: 0.0009\n","Train Epoch: 95 3000\tLoss: 0.0015\n","Train Epoch: 95 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 96 0\tLoss: 0.0015\n","Train Epoch: 96 1000\tLoss: 0.0019\n","Train Epoch: 96 2000\tLoss: 0.0009\n","Train Epoch: 96 3000\tLoss: 0.0015\n","Train Epoch: 96 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 97 0\tLoss: 0.0015\n","Train Epoch: 97 1000\tLoss: 0.0019\n","Train Epoch: 97 2000\tLoss: 0.0009\n","Train Epoch: 97 3000\tLoss: 0.0015\n","Train Epoch: 97 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 98 0\tLoss: 0.0015\n","Train Epoch: 98 1000\tLoss: 0.0019\n","Train Epoch: 98 2000\tLoss: 0.0009\n","Train Epoch: 98 3000\tLoss: 0.0015\n","Train Epoch: 98 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 99 0\tLoss: 0.0015\n","Train Epoch: 99 1000\tLoss: 0.0019\n","Train Epoch: 99 2000\tLoss: 0.0009\n","Train Epoch: 99 3000\tLoss: 0.0015\n","Train Epoch: 99 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n","Train Epoch: 100 0\tLoss: 0.0015\n","Train Epoch: 100 1000\tLoss: 0.0019\n","Train Epoch: 100 2000\tLoss: 0.0009\n","Train Epoch: 100 3000\tLoss: 0.0015\n","Train Epoch: 100 4000\tLoss: 0.0008\n","---------------------------------------------\n","\n","Test set: Average loss: 0.0011, Accuracy: 500/500 (100.00%)\n","\n"]}],"source":["# import torch libraries\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","\n","## Define our neural network class\n","torch.manual_seed(42)\n","class NN(nn.Module):\n","    def __init__(self):\n","        super(NN, self).__init__()\n","        self.dense1 = nn.Linear(2, 2)\n","        self.dense2 = nn.Linear(2, 1)\n","    def forward(self, x):\n","        x = F.tanh(self.dense1(x))\n","        x = self.dense2(x)\n","        return torch.squeeze(x)\n","\n","\n","# initialize our network\n","model = NN()\n","lr = 0.001\n","\n","## optimizer = stochastic gradient descent\n","optimizer = optim.SGD(model.parameters(), lr)\n","\n","## train and test functions\n","def train(epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_data):\n","        data, target = Variable(torch.from_numpy(data)), Variable(torch.from_numpy(target))\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.mse_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 100 == 0:\n","            print('Train Epoch: {} {}\\tLoss: {:.4f}'.format(epoch, batch_idx * len(data), loss.item()))\n","def test():\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    for data, target in test_data:\n","        data, target = Variable(torch.from_numpy(data), volatile=True), Variable(torch.from_numpy(target))\n","        output = model(data)\n","        test_loss += F.mse_loss(output, target)\n","        correct += (np.around(output.data.numpy()) == np.around(target.data.numpy())).sum()\n","    test_loss /= len(test_data)\n","    test_loss = test_loss.item()\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        test_loss, correct, batch_size * len(test_data), 100. * correct / (batch_size * len(test_data))) )\n","    \n","## run experiment \n","nepochs = 100\n","lr = 0.001\n","\n","print('lr=', lr)\n","for epoch in range(1, nepochs + 1):\n","    train(epoch)\n","    print('---------------------------------------------')\n","    test()"]},{"cell_type":"markdown","metadata":{"id":"P1d47bLhDMlw"},"source":["# Submission Instructions\n","Once you are finished, follow these steps:\n","\n","Restart the kernel and re-run this notebook from beginning to end by going to Kernel > Restart Kernel and Run All Cells.\n","If this process stops halfway through, that means there was an error. Correct the error and repeat Step 1 until the notebook runs from beginning to end.\n","Double check that there is a number next to each code cell and that these numbers are in order.\n","Then, submit your lab as follows:\n","\n","Go to File > Print > Save as PDF.\n","Double check that the entire notebook, from beginning to end, is in this PDF file. Make sure Solution for Exercise 5 are in for marks. \n","Upload the PDF to Spectrum. "]},{"cell_type":"markdown","metadata":{"id":"4FBd4KLZwyVB"},"source":["# Acknowledgement\n","\n","Some of the works are inspired from \n","1. Effect of learning rate on AI model = https://www.commonlounge.com/discussion/5076b2cfb2364594ba608fca3ac606bb"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1WsERLRp6JBKLNroJ9vpQcqiEzG4HKK6G","timestamp":1667908321390}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}